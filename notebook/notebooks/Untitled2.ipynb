{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc5b319-cc01-4bf0-8f17-fda5caa8ff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [number: bigint]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(500).toDF(\"number\")\n",
    "df.select(df.col(\"number\") + 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59dccac-d7f9-40b0-81de-bdac7eb75649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([0], [1])\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).toDF().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1167997-c9ab-4168-b00c-fdeefa4a5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4262e6-7608-4b28-b34d-433efd102aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5387f71-0fb6-43bd-bfc4-56c521937aed",
   "metadata": {},
   "source": [
    "## Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853a9e73-7350-4400-a2c6-30ad32154d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"json\").load(\"./flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58deae9-f2e2-41e3-b697-caa558efb465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"./flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66b9e2-ba30-4eab-8866-73552608e043",
   "metadata": {},
   "source": [
    "스키마는 여러 개의 StructField 타입 필드로 구성된 StructType 객체입니다.\n",
    "스파크는 런타임에 데이터 타입이 스키마 데이터 타입과 일치하지 않으면 오류를 발생시킵니다 (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35762afd-2ea0-49f3-ba66-f84d5571278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "//import org.apache.spark.sql.Metadata\n",
    "\n",
    "val myManualSchema = StructType(Array(\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "    StructField(\"count\", LongType, false, Metadata.fromJson(\"{\\\"hello\\\": \\\"world\\\"}\"))))\n",
    "\n",
    "val df = spark.read.format(\"json\").schema(myManualSchema).load(\"./flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d7ca35-a42e-4f53-a58e-c37daa354563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|\n",
      "+------------------------+-------------------+-----+\n",
      "|United States           |Romania            |15   |\n",
      "|United States           |Croatia            |1    |\n",
      "|United States           |Ireland            |344  |\n",
      "|Egypt                   |United States      |15   |\n",
      "|United States           |India              |62   |\n",
      "|United States           |Singapore          |1    |\n",
      "|United States           |Grenada            |62   |\n",
      "|Costa Rica              |United States      |588  |\n",
      "|Senegal                 |United States      |40   |\n",
      "|Moldova                 |United States      |1    |\n",
      "|United States           |Sint Maarten       |325  |\n",
      "|United States           |Marshall Islands   |39   |\n",
      "|Guyana                  |United States      |64   |\n",
      "|Malta                   |United States      |1    |\n",
      "|Anguilla                |United States      |41   |\n",
      "|Bolivia                 |United States      |30   |\n",
      "|United States           |Paraguay           |6    |\n",
      "|Algeria                 |United States      |4    |\n",
      "|Turks and Caicos Islands|United States      |230  |\n",
      "|United States           |Gibraltar          |1    |\n",
      "+------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c9c8f39-e901-4079-9559-acf6142c6447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, column}\n",
       "res9: org.apache.spark.sql.Column = myColumn\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, column}\n",
    "\n",
    "col(\"myCol\")\n",
    "column(\"myColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299aa2b6-9801-427a-afa9-b8099855b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "컬럼은 컬럼명을 저장된 정보와 비교하기 전 미확인 상태로 남는다\n",
    "\n",
    "추가 팁 : 스칼라는 심벌을 사용해 특정 식별자를 참조할 때 사용\n",
    "$\"myColumn\" `myColumn 같은 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45427b6-e1f0-475e-b93a-c541a6f364c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n",
       "res13: org.apache.spark.sql.Column = ((myCol + 5) + 200)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "expr(\"myCol\")\n",
    "expr(\"(myCol + 5) + 200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63db9450-ecfe-4d11-8d23-d723504febd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "myRow: org.apache.spark.sql.Row = [Hello,null,1,false]\n",
       "res18: Int = 1\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val myRow = Row(\"Hello\", null, 1, false)\n",
    "myRow(0)\n",
    "\n",
    "//explicit 하게 데이터 타입 명시 필요\n",
    "myRow(0).asInstanceOf[String]\n",
    "myRow.getInt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82fb1f43-04a8-47ed-968d-eca305d05265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"./flight-data/json/2015-summary.json\")\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7df33-394f-449f-83a7-7cb49572e4c7",
   "metadata": {},
   "source": [
    "spark 은 Seq 타입에 toDF 를 사용할 수 있고 implicit 타입의 장점을 얻을 수 있다.\n",
    "하지만 null과 잘 맞지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3cd6f-d39b-4dfd-840f-eda2311c3cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
